#+Title: DHBB 

* processing the dhbb/text files

O comando abaixo identifica arquivos com BOM. Para os arquivos
listados, vc pode confirmar o encoding 'UTF-8 Unicode (with BOM) text'
com o comando =file= e remover com o comando =dos2unix=.

#+BEGIN_SRC sh
egrep -c "^---$" *.text | awk -F : '$2 == 1 { print }'
#+END_SRC

https://stackoverflow.com/questions/1068650/using-awk-to-remove-the-byte-order-mark

#+BEGIN_SRC bash
for f in $(find ../raw -size 0); do BN=$(basename $f .raw); awk 'NR==1{sub(/^\xef\xbb\xbf/,"")}1' $BN.text > $BN.new; done
#+END_SRC

Para separar metadados:

#+BEGIN_SRC sh
  for f in *.text; do
      awk 'BEGIN { text=0; } text<=1 {print} /^---$/ { text = text + 1; }' $f > $(basename $f .text).meta ;
  done
#+END_SRC

* raw directory

Execute `make`, o arquivo `Makefile` controla o processo chamando o `extract.awk`.

* sentenças

Na pasta =sents/= executar o comando =make= após estar em virtual
environment do python que tenha a biblioteca NLTK instalada.

Para cada arquivo em =raw/= temos um arquivo =.sent-= e um arquivo
=.diff= se ferramentas de segmentação discordarem na segmentação de
sentenças do respectivo arquivo raw.

Os arquivos `.sent` contém spans indicadores de inicio de fim de
sentenças, dois números por linha, indicando o segmento:
=[start,end)=. Os char-based spans são referentes a uma determinada
versão do `.raw`. Se o arquivo `.raw` é alterado, os respectivos
`.sent` ficam inválidos. Também ficam nesta pasta arquivos `.diff` que
contem além do span, uma lista de inicias indicando as ferramentas que
produziram os respectivos span.

** freeling (deprecated)

Para gerar os arquivos, rode nesta pasta o comando abaixo após 
iniciar o servidor freeling (vide abaixo).

1) open Freeling server on another terminal:

$ cd etc
$ analyze -f ./pt.cfg --output json --noflush --server --port 8000

(note: no arquivo pt.cfg em etc, temos referencias aos arquivos
splitter.dat e tokenizer.dat)

** treino modelo OpenNLP

No diretório sent:

1. `.offset` arquivos com begin/end para cada sentença do respectivo arquivo raw
2. `.goffset` golden offset onde todos os begin/end foram conferidos
3. `.sent` arquivos temporarios com uma sentença por linha (criados via descompactacao de .offset ou .goffset)
4. `.diff` arquivos praticamente iguais aos .offset ou .goffset mas
   contendo id da ferramenta que produziu o begin/end

um ciclo de revisao de quebras de sentenças em um arquivo:

1. sent-descompat 45.offset > 45.sent
2. sent-compact 45.sent > 45.goffset
3. apaga 45.offset e 45.sent

para treinar:

1. for f in *.goffset; sent-descompat $f >> golden.sent; done
2. chamar openNLP para treinar com golden.sent
3. remover golden.sent

Executar:

: src/train-opennlp.sh

Este script basicamente executa:

Os arquivos pertinentes ao treino do modelo são:

sentences_gold.sent
src/params.opennlp

O arquivo amostras_DHBB.sent contém uma amostra de aproximadamente 200
frases do DHBB onde o modelo pré treinado do Bosque errou a
segmentação O arquivo param.txt possui os parâmetros do treino. A
ideia é unir essas frases ao Bosque e retreinar o modelo.

Para tal, proceda da seguinte maneira, faça o download do arquivo de
treino do Bosque:

: wget https://github.com/UniversalDependencies/UD_Portuguese-Bosque/raw/master/pt_bosque-ud-train.conllu

Utilize o conversor do OpenNLP para extrair as sentenças do arquivo
.conllu de treino do Bosque.

: opennlp SentenceDetectorConverter conllu -sentencesPerSample 10 -data pt_bosque-ud-train.conllu > bosque_train.sent

Agora una os dois arquivos 

: cat bosque_train.sent amostras_dhbb.sent > arquivo_treino.sent

Use o arquivo arquivo_treino.sent para treinar o modelo

: opennlp SentenceDetectorTrainer -lang portuguese -model bosque_model.opennlp -data arquivo_treino.sent -encoding utf-8 -params param.txt

Remova os arquivos não mais necessários

: rm amostras_dhbb.sent arquivo_treino.sent bosque_train.sent pt_bosque-ud-train.conllu 

** Python NLTK

  Criar um ambiente virtual para o Python e activar ele. Vc pode ter um
  ambiente específico para o projeto ou algum ambiente compartilhado no
  seu home.

  https://docs.python.org/3/tutorial/venv.html

  Instalar o NLTK depois de activar o virtual environment:

  source bin/activate
  pip install nltk

  Carregar o virtual environment do python (com python3, nltk
  dependencies)

  $ source ~/env/bin/active

 treino NLTK Punkt?

 inspecionar modelo Punkt?

 
* UDP directory

awk -F \: 'BEGIN {OFS="|"} {n=split($2, arr, /\|/); for (i=1;i<=n;++i) print $1, NR, arr[i] }' nomes.txt > nomes-splited.txt

#+BEGIN_SRC bash
time find ~/work/cpdoc/dhbb/raw -name "*.raw" | xargs ./udpipe --tokenize --tag --parse --outfile=../../cpdoc/dhbb/udp/{}.conllu ../udpipe-ud-2.0-conll17-170315/models/portuguese-ud-2.0-conll17-170315.udpipe
#+END_SRC
     
* Links

- http://pyyaml.org/wiki/PyYAMLDocumentation
- http://www.w3.org/TR/rdf-sparql-query/

* Next Stesp

 - Issue 
 - https://stackoverflow.com/questions/47274540/how-to-improve-sentence-segmentation-of-nltk

