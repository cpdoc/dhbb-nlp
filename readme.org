#+Title: DHBB 

* processing the dhbb/text files

O comando abaixo identifica arquivos com BOM. Para os arquivos
listados, vc pode confirmar o encoding 'UTF-8 Unicode (with BOM) text'
com o comando =file= e remover com o comando =dos2unix=.

#+BEGIN_SRC sh
egrep -c "^---$" *.text | awk -F : '$2 == 1 { print }'
#+END_SRC

https://stackoverflow.com/questions/1068650/using-awk-to-remove-the-byte-order-mark

#+BEGIN_SRC bash
for f in $(find ../raw -size 0); do BN=$(basename $f .raw); awk 'NR==1{sub(/^\xef\xbb\xbf/,"")}1' $BN.text > $BN.new; done
#+END_SRC

Para separar metadados:

#+BEGIN_SRC sh
  for f in *.text; do
      awk 'BEGIN { text=0; } text<=1 {print} /^---$/ { text = text + 1; }' $f > $(basename $f .text).meta ;
  done
#+END_SRC

* The raw directory

Execute `make`, o arquivo `Makefile` controla o processo chamando o `extract.awk`.

* UDP directory

awk -F \: 'BEGIN {OFS="|"} {n=split($2, arr, /\|/); for (i=1;i<=n;++i) print $1, NR, arr[i] }' nomes.txt > nomes-splited.txt

#+BEGIN_SRC bash
time find ~/work/cpdoc/dhbb/raw -name "*.raw" | xargs ./udpipe --tokenize --tag --parse --outfile=../../cpdoc/dhbb/udp/{}.conllu ../udpipe-ud-2.0-conll17-170315/models/portuguese-ud-2.0-conll17-170315.udpipe
#+END_SRC

* Dictionaries

#+BEGIN_SRC lisp
(ql:quickload :cpdoc)
(in-package :cpdoc)

(mapcar (lambda (file)
		 (let ((bd (extract-meta file)))
		   (list (pathname-name file)
			 (gethash "natureza" bd)
			 (gethash "title" bd))))
	       (directory "text/*.text"))

(with-open-file (file "dic/titles-from-meta.text" :direction :output :if-exists :supersede)
	 (format file "~{~{~a~^|~}~%~}" *))
#+END_SRC
     
* Links

- http://pyyaml.org/wiki/PyYAMLDocumentation
- http://www.w3.org/TR/rdf-sparql-query/

* Python NLTK

Criar um ambiente virtual para o Python e activar ele. Vc pode ter um
ambiente específico para o projeto ou algum ambiente compartilhado no
seu home.

https://docs.python.org/3/tutorial/venv.html

Instalar o NLTK depois de activar o virtual environment:

source bin/activate
pip install nltk

** treino NLTK Punkt

...

** inspecionar modelo Punkt

...

* Sentences

Note que os arquivos `.sent` contém spans indicadores de inicio de fim
de sentenças, dois números por linha, indicando o segmento:
=[start,end)=. Os char-based spans são referentes a uma determinada
versão do `.raw`. Se o arquivo `.raw` é alterado, os respectivos
`.sent` ficam inválidos.

Nesta pasta, contém os arquivos .sent onde tanto Freeling quanto
OpenNLP convergiram em sua segmentação, ou seja, onde temos uma alta
probabilidade de serem segmentações corretas.

Para gerar os arquivos, rode nesta pasta o comando abaixo após 
iniciar o servidor freeling (vide abaixo).

1) open Freeling server on another terminal:

$ cd etc
$ analyze -f ./pt.cfg --output json --noflush --server --port 8000

(note: no arquivo pt.cfg em etc, temos referencias aos arquivos
splitter.dat e tokenizer.dat)

2) carregar o virtual environment do python (com python3, nltk dependencies)

$ source ~/env/bin/active

3) rodar abcl Lisp

$ ./make.sh

deprecated code:

#+BEGIN_SRC batch
$ sbcl
(load "splitter.lisp")
(sb-ext:save-lisp-and-die "splitter" :toplevel #'main :executable t)
#+END_SRC

Configure Freling
- the file splitter.dat must add the markers « »
- the file tokenizer.dat must remove the abbreviation "pt."

** training the model

Os arquivos pertinentes ao treino do modelo são:

sentences_gold.sent
param.txt

O arquivo amostras_DHBB.sent contém uma amostra de aproximadamente 200
frases do DHBB onde o modelo pré treinado do Bosque errou a
segmentação O arquivo param.txt possui os parâmetros do treino. A
ideia é unir essas frases ao Bosque e retreinar o modelo.

Para tal, proceda da seguinte maneira, faça o download do arquivo de
treino do Bosque:

: wget https://github.com/UniversalDependencies/UD_Portuguese-Bosque/raw/master/pt_bosque-ud-train.conllu

Utilize o conversor do OpenNLP para extrair as sentenças do arquivo
.conllu de treino do Bosque.

: opennlp SentenceDetectorConverter conllu -sentencesPerSample 10 -data pt_bosque-ud-train.conllu > bosque_train.sent

Agora una os dois arquivos 

: cat bosque_train.sent amostras_dhbb.sent > arquivo_treino.sent

Use o arquivo arquivo_treino.sent para treinar o modelo

: opennlp SentenceDetectorTrainer -lang portuguese -model bosque_model.opennlp -data arquivo_treino.sent -encoding utf-8 -params param.txt

Remova os arquivos não mais necessários

: rm amostras_dhbb.sent arquivo_treino.sent bosque_train.sent pt_bosque-ud-train.conllu 

Ou rode

: make_model.sh path/to/opennlp

* Next Stesp

 - Issue 
 - https://stackoverflow.com/questions/47274540/how-to-improve-sentence-segmentation-of-nltk

