#+Title: DHBB 

* processing the dhbb/text files

O comando abaixo identifica arquivos com BOM. Para os arquivos
listados, vc pode confirmar o encoding 'UTF-8 Unicode (with BOM) text'
com o comando =file= e remover com o comando =dos2unix=.

#+BEGIN_SRC sh
egrep -c "^---$" *.text | awk -F : '$2 == 1 { print }'
#+END_SRC

https://stackoverflow.com/questions/1068650/using-awk-to-remove-the-byte-order-mark

#+BEGIN_SRC bash
for f in $(find ../raw -size 0); do BN=$(basename $f .raw); awk 'NR==1{sub(/^\xef\xbb\xbf/,"")}1' $BN.text > $BN.new; done
#+END_SRC

Para separar metadados:

#+BEGIN_SRC sh
  for f in *.text; do
      awk 'BEGIN { text=0; } text<=1 {print} /^---$/ { text = text + 1; }' $f > $(basename $f .text).meta ;
  done
#+END_SRC

* raw directory

Executar o `make`, o arquivo `Makefile` controla o processo chamando o
`extract.awk`. 

* sentenças

Na pasta =sents/= executar o comando =make= após estar em virtual
environment do python que tenha a biblioteca NLTK instalada.

Para cada arquivo em =raw/= temos um arquivo =.sent-= e um arquivo
=.diff= se ferramentas de segmentação discordarem na segmentação de
sentenças do respectivo arquivo raw.

Os arquivos `.sent` contém spans indicadores de inicio de fim de
sentenças, dois números por linha, indicando o segmento:
=[start,end)=. Os char-based spans são referentes a uma determinada
versão do `.raw`. Se o arquivo `.raw` é alterado, os respectivos
`.sent` ficam inválidos. Também ficam nesta pasta arquivos `.diff` que
contem além do span, uma lista de inicias indicando as ferramentas que
produziram os respectivos span.

** freeling (deprecated)

Para gerar os arquivos, rode nesta pasta o comando abaixo após 
iniciar o servidor freeling (vide abaixo).

1) open Freeling server on another terminal:

$ cd etc
$ analyze -f ./pt.cfg --output json --noflush --server --port 8000

(note: no arquivo pt.cfg em etc, temos referencias aos arquivos
splitter.dat e tokenizer.dat)


* UDP directory

awk -F \: 'BEGIN {OFS="|"} {n=split($2, arr, /\|/); for (i=1;i<=n;++i) print $1, NR, arr[i] }' nomes.txt > nomes-splited.txt

#+BEGIN_SRC bash
time find ~/work/cpdoc/dhbb/raw -name "*.raw" | xargs ./udpipe --tokenize --tag --parse --outfile=../../cpdoc/dhbb/udp/{}.conllu ../udpipe-ud-2.0-conll17-170315/models/portuguese-ud-2.0-conll17-170315.udpipe
#+END_SRC
     
* Links

- http://pyyaml.org/wiki/PyYAMLDocumentation
- http://www.w3.org/TR/rdf-sparql-query/

* Next Stesp

 - Issue 
 - https://stackoverflow.com/questions/47274540/how-to-improve-sentence-segmentation-of-nltk

